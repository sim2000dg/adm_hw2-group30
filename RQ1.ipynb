{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "os.chdir(\"/Volumes/ExtraHDD2/DS_Assignments_Data/ADM_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the datasets\n",
    "# Also converting the column cts in posters to date/time type. This will be useful for later stages\n",
    "posts = pd.read_csv(\"instagram_posts.csv\", delimiter=\"\\t\", parse_dates=[5], nrows=10000)\n",
    "profiles = pd.read_csv(\"instagram_profiles.csv\", delimiter=\"\\t\")\n",
    "locations = pd.read_csv(\"instagram_locations.csv\", delimiter=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# RQ1 - Basic EDA [H1]\n",
    "# Let's start with a *very* basic summary of the features of the datasets. On a very coarse level of analysis, we can check the number of observations in each dataset. _Posts is definitely the bigger one (challenging especially for our memory), with 42'710'197 observations, followed by _profiles_ (with 4'509'586 observations) and _locations_ (with 1'022'658 observations).\n",
    "# This makes sense considering that for every profile there will be n posts and considering that locations get repeated a lot among posts, with many posts not even having any location. Specifically, to prove the point, locations repeat on average 17 times in the _post_ dataset and there are 12'972'772 posts without locations as can be seen in the code.\n",
    "\n",
    "# Number of observations for dataset\n",
    "print(f\"Number of obs posts:{len(posts)}\")\n",
    "print(f\"Number of obs profiles: {len(profiles)}\")\n",
    "print(f\"Number of obs locations: {len(locations)}\")\n",
    "\n",
    "# Number of non-complete observations for dataset\n",
    "# Is null returns a dataframe of booleans (na or not for each entry). Any returns true for each row (axis=1) which has a true value (a na field) and sums over the booleans, considering False as 0 and True as 1. This is a solution which is more elegant with respect to others.\n",
    "print(f\"Number of non-complete rows in posts: {posts.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Number of non-complete rows in profiles: {profiles.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Number of non-complete rows in locations: {locations.isnull().any(axis=1).sum()}\")\n",
    "\n",
    "# Number of posts without locations\n",
    "print(\"Number of posts without locations: {}\".format(posts[\"location_id\"].isnull().sum()))\n",
    "# Mean number of posts for each location (2 decimal digits)\n",
    "print(\"Mean number of repetitions for each location: {}\".format(round(posts[\"location_id\"].value_counts().mean())))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features of the post dataset [H2]\n",
    "# First step in any proper EDA is looking at the columns of the tables that we are using. Luckily the guy who uploaded the dataset on Kaggle was also very clear in describing the features.  Let's start with the _posts_ dataset, the first and most important one:\n",
    "# - The _sid_profile_ feature acts as foreign key to link the _posts_ dataset to the _profiles_ dataset, allowing joins.\n",
    "# - The post_id is the ID of the post itself, nothing particularly important to say: it is just a key of the table.\n",
    "# - The _profile_id_ is a foreign key similar to _sid_profile_ the difference seems to be that _profile_id_ may be null, while _sid_profile_ seems a more reliable foreign key. We do not know why since we were not the ones who retrieved the data.\n",
    "# - The _location_id_ is the foreign key that links the _posts_ dataset to the _location_ one.\n",
    "# - _cts_ is the timestamp of the post (which was converted to the Numpy _datetime64_ dtype).\n",
    "# - _post_type_ is a categorical variable (1, 2, 3) which tells us if the post was a photo, a video or multimedia.\n",
    "# - _numbr_likes_ is the number of likes.\n",
    "# - _number_comments_ is the number of comments.\n",
    "\n",
    "print(posts.columns.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The only numerical variables for the _posts_ dataset are the number of likes, the number of comments. We print the basic summary statistics (mean, standard deviation, maximum, minimum and quartiles) for these variables in markdown format in order to put them into the notebook.\n",
    "\n",
    "# Print the basic summary statistics for the numeric variables\n",
    "print(posts.describe().iloc[1:, 5:].to_markdown())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Boxplot numerical of posts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# At this point, in this preliminary EDA, a cool stuff that we can do is plotting the mean number of posts per weekday. We can do that quite easily by counting the observations using the weekday as grouping key and then normalizing the count with the number of weeks in the dataset.\n",
    "number_of_weeks = len(posts[\"cts\"].dt.isocalendar().iloc[:, 0:2].drop_duplicates())\n",
    "grouped_dayweek_post = posts.groupby(posts[\"cts\"].dt.dayofweek)\n",
    "week_list = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "count_week = grouped_dayweek_post[\"sid\"].count().set_axis(week_list)\n",
    "# This normalizes the number of posts giving us the mean number post for each day (across the years)\n",
    "count_week /= number_of_weeks\n",
    "count_week.plot(marker=\"o\")\n",
    "plt.xlabel(\"Day of week\")\n",
    "plt.ylabel(\"# of posts\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features of the post dataset [H2]\n",
    "print(profiles.columns.values)\n",
    "\n",
    "# Basic summary statistics for profiles\n",
    "profiles.describe().iloc[1:, 2:].drop(\"min\", axis = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Boxplots of numerical features in profiles\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "profiles[[\"following\", \"followers\"]].plot.box(showfliers=False, vert=False, ax=ax, fontsize = \"large\")\n",
    "ax.set_title(\"Boxplots for # of followers and profiles followed per user\", {\"fontsize\":18})\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "profiles[\"n_posts\"].plot.box(showfliers=False, ax = ax, vert = False, fontsize = \"large\")\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_title(\"Boxplots for # of posts per user\", {\"fontsize\":18})\n",
    "plt.suptitle(\"Boxplots for the numerical features of the profiles dataset\", fontsize = 40)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
