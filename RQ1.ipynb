{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import sklearn.cluster as clustering\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "os.chdir(\"/Users/simonedigregorio/Downloads/archive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets\n",
    "# Also converting the column cts in posters to date/time type. This will be useful for later stages\n",
    "posts = pd.read_csv(\"instagram_posts.csv\", delimiter=\"\\t\", parse_dates=[5], infer_datetime_format = True)\n",
    "profiles = pd.read_csv(\"instagram_profiles.csv\", delimiter=\"\\t\")\n",
    "locations = pd.read_csv(\"instagram_locations.csv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1 - Basic EDA [H1]\n",
    "# Let's start with a *very* basic summary of the features of the datasets. On a very coarse level of analysis, we can check the number of observations in each dataset. _Posts is definitely the bigger one (challenging especially for our memory), with 42'710'197 observations, followed by _profiles_ (with 4'509'586 observations) and _locations_ (with 1'022'658 observations).\n",
    "# This makes sense considering that for every profile there will be n posts and considering that locations get repeated a lot among posts, with many posts not even having any location. Specifically, to prove the point, locations repeat on average 17 times in the _post_ dataset and there are 12'972'772 posts without locations as can be seen in the code (this partially answers a point of RQ2). Also, there are some inconsistencies in the datasets (posts from profiles not present in the _profile_ dataset or posts with tagged locations not present in the _locations_ dataset), so what we are saying here is approximate at best.\n",
    "\n",
    "# Number of observations for dataset\n",
    "print(f\"Number of obs posts:{len(posts)}\")\n",
    "print(f\"Number of obs profiles: {len(profiles)}\")\n",
    "print(f\"Number of obs locations: {len(locations)}\")\n",
    "\n",
    "# Number of non-complete observations for dataset\n",
    "# Is null returns a dataframe of booleans (na or not for each entry). Any returns true for each row (axis=1) which has a true value (a na field) and sums over the booleans, considering False as 0 and True as 1. This is a solution which is more elegant with respect to others.\n",
    "print(f\"Number of non-complete rows in posts: {posts.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Number of non-complete rows in profiles: {profiles.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Number of non-complete rows in locations: {locations.isnull().any(axis=1).sum()}\")\n",
    "\n",
    "# Number of posts without locations\n",
    "print(\"Number of posts without locations: {}\".format(posts[\"location_id\"].isnull().sum()))\n",
    "# Mean number of posts for each location (2 decimal digits)\n",
    "print(\"Mean number of repetitions for each location: {}\".format(round(posts[\"location_id\"].value_counts().mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of the post dataset [H2]\n",
    "# First step in any proper EDA is looking at the columns of the tables that we are using. Luckily the guy who uploaded the dataset on Kaggle was also very clear in describing the features. Let's start with the _posts_ dataset, the first and most important one:\n",
    "# - The _sid_profile_ feature acts as something similar to a foreign key to link the _posts_ dataset to the _profiles_ dataset, allowing joins. -1 is the id for profiles not present in the _profiles_ dataset.\n",
    "# - The post_id is the ID of the post itself, nothing particularly important to say: it is just a key of the table.\n",
    "# - The _profile_id_ is the instagram ID of the profile which published the post.\n",
    "# - The _location_id_ is the foreign key that links the _posts_ dataset to the _location_ one.\n",
    "# - _cts_ is the timestamp of the post.\n",
    "# - _post_type_ is a categorical variable (1, 2, 3) which tells us if the post was a photo, a video or multimedia.\n",
    "# - _numbr_likes_ is the number of likes.\n",
    "# - _number_comments_ is the number of comments.\n",
    "\n",
    "print(posts.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only numerical variables for the _posts_ dataset are the number of likes, the number of comments. We print the basic summary statistics (mean, standard deviation, maximum, minimum and quartiles) for these variables in markdown format in order to put them into the notebook. From the table, we can already notice some important features of the distribution. For example, the variances are **very** high and the quartiles are all lower than the mean values. This specific last information tells us that the distribution is strongly right-tailed.\n",
    "# More specifically, we can check that numerically (and statistically) by computing the skewness and the kurtosis (third and four standardized moment of the sample distribution).\n",
    "# The skewness is in both cases **strongly** positive (very strong right tail indeed), which tells us that the mass is concentrated on the far left of the distro. This would be enough, but in order to make it more clear notice also the excess kurtosis (w.r.t. to the normal).\n",
    "# These are extremely distorted distributions (the one for comments far more distorted than the one for likes), which makes sense since in a social network environment we expect the directed graph of connections/interactions to follow some kind of preferential attachment process.\n",
    "\n",
    "# Print the basic summary statistics for the numeric variables\n",
    "print(posts.describe().iloc[1:, 5:].to_markdown(), end = \"\\n\\n\")\n",
    "\n",
    "# Print the skewness and the kurtosis for likes and comments\n",
    "print(\"Skewness for likes distro: {}\\nSkewness for comments distro: {}\".format(round(posts[\"numbr_likes\"].skew(), 2), round(posts[\"number_comments\"].skew(), 2)))\n",
    "print(\"Excess kurtosis for likes distro: {}\\nExcess kurtosis for comments distro: {}\".format(round(posts[\"numbr_likes\"].kurtosis(), 2), round(posts[\"number_comments\"].kurtosis(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A way to visualize distributions so distorted would be to simply avoid considering the outliers and concentrate on the quartiles of the distribution. This can be done by plotting the boxplots considering just the quartiles and the whiskers (the right whisker is the 3rd quartile + 1.5 IQR) and avoiding the outliers. Showfliers = False in the Pandas utility method (which in turn calls matplotlib) is enough to make the plot readable. As can be seen in the table, look at how low is the value of the points in the interquartile range.\n",
    "\n",
    "# Boxplot numerical of posts\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "posts[\"numbr_likes\"].plot.box(showfliers=False, vert=False, ax=ax, fontsize = \"large\")\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_title(\"Boxplot for # of likes per post\", {\"fontsize\":18})\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "posts[\"number_comments\"].plot.box(showfliers=False, vert=False, ax=ax, fontsize = \"large\")\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_title(\"Boxplot for # of comments per post\", {\"fontsize\":18})\n",
    "plt.suptitle(\"Boxplots for the numerical features of the posts dataset\", fontsize = 40)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Another way would be to plot the histogram frequencies on a logarithmic scale. (Another idea I had by talking with a colleague). +1 was inserted to avoid the problem due to the logarithm not defined in 0.\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "(posts[\"numbr_likes\"]+1).apply(np.log).hist(ax = ax, bins = 30)\n",
    "ax.set_xlabel(\"Number of likes + 1 (log scale)\")\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "(posts[\"number_comments\"]+1).apply(np.log).hist(ax = ax, bins = 30)\n",
    "ax.set_xlabel(\"Number of comments + 1 (log scale)\")\n",
    "plt.suptitle(\"Histograms (log scale) of numerical features in posts\", fontsize = 40)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# It seems that the distribution of likes is approximately log-normal, i.e. the logarithm of the number of likes is normally distributed. A way to test this would be a quantile-quantile plot. Let's plot that.\n",
    "# From the plot, we can see that it definitely seems log-normal. The quantiles of the (standardized) sample distribution of the log number of likes almost perfectly match with the ones of a standard normal.\n",
    "\n",
    "quantiles_stdnorm = [stats.norm.ppf(x) for x in np.arange(0.05, 0.96, 0.05)]\n",
    "\n",
    "log_transformed_likes = (posts[\"numbr_likes\"]+1).apply(np.log)\n",
    "sample_quantiles_likes = ((log_transformed_likes-log_transformed_likes.mean())/log_transformed_likes.std()).quantile(np.arange(0.05, 0.96, 0.05))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.suptitle(\"  Q-Q plot for log-distribution of the number of likes\", fontsize = 30)\n",
    "plt.title(\"Standard normal quantiles vs. quantiles of standardized sample distribution\", fontsize = 20, y = 1.02)\n",
    "ax = plt.gca()\n",
    "ax.plot(quantiles_stdnorm, quantiles_stdnorm)\n",
    "ax.scatter(quantiles_stdnorm, sample_quantiles_likes)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# At this point, in this preliminary EDA, a cool stuff that we can do is plotting the mean number of posts per weekday. We can do that quite easily by counting the observations using the weekday as grouping key and then normalizing the count with the number of weeks in the dataset.\n",
    "# As one could expect, the weekend sees the number of posts skyrocketing (less working hours, more trips/holidays, stronger presence of the users online).\n",
    "plt.figure(figsize = (12, 8))\n",
    "number_of_weeks = len(posts[\"cts\"].dt.isocalendar().iloc[:, 0:2].drop_duplicates())\n",
    "grouped_dayweek_post = posts.groupby(posts[\"cts\"].dt.dayofweek)\n",
    "week_list = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "count_week = grouped_dayweek_post[\"sid\"].count().set_axis(week_list)\n",
    "# This normalizes the number of posts giving us the mean number post for each day (across the years)\n",
    "count_week /= number_of_weeks\n",
    "count_week.plot(marker=\"o\")\n",
    "plt.xlabel(\"Day of week\")\n",
    "plt.ylabel(\"# of posts\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features of the profile dataset [H2]\n",
    "# Again, as we did before, let's recap the columns in the DataFrame:\n",
    "# - _sid_ is the key which is used as reference for the foreign key in _posts_\n",
    "# - profile_id is another id (the Instagram one).\n",
    "# - profile_name nomen omen.\n",
    "# - _firstname_lastname_ nomen omen.\n",
    "# - _description_ is the description of the profile. Looking at the actual strings, it is the \"bio\" of the profile.\n",
    "# - _following_ is the number of accounts followed by the users.\n",
    "# - _followers_ is the number of accounts which follow the user.\n",
    "# - _n_posts_ is the number of posts published by the user.\n",
    "# - _url_ is the URL highlighted in the profile under the description/bio.\n",
    "# - _cts_ is the timestamp for the time when the data of the profile was scraped.\n",
    "# - _is_business_account_ is a boolean to check if the profile is a business account or not.\n",
    "\n",
    "print(profiles.columns.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The only numerical columns here are _following_, _followers_ and n_posts. Let's look at their distribution by printing summary statistics and drawing some boxplots. Especially for what concerns the followers, what we see is basically the quintessence of what we explained in the previous section of this question.\n",
    "# However, since RQ8 specifically deals with the followers' distribution, we will not go in depth with that now.\n",
    "\n",
    "# Let's jump to the number of posts for now. The distribution for the number of posts seems far less distorted than the one of followers, and this is because the number of posts is not something **strictly** related to the popularity of the user, which is in turn modelled by a preferential attachment process. Again, this is intuitive: a user may enjoy interacting with its relatively small subset of followers and publish a lot, maybe at the same or at a similar rate of an influencer.\n",
    "# The distortion (anyway present) highlighted by skewness and kurtosis can be easily explained (at least in part) by the fact that a social network is full of zombie accounts: bots, inactive users and people subscribing just to follow other people.\n",
    "\n",
    "# Basic summary statistics for profiles\n",
    "print(profiles.describe().iloc[1:, 2:].drop(\"min\", axis = 0).to_markdown(), end = \"\\n\\n\")\n",
    "\n",
    "print(\"Skewness for # of posts distro: {}\".format(round(profiles[\"n_posts\"].skew(), 2)))\n",
    "print(\"Excess kurtosis for # of posts distro: {}\".format(round(profiles[\"n_posts\"].kurtosis(), 2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Boxplots of numerical features in profiles\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "profiles[[\"following\"]].plot.box(showfliers=False, fontsize = \"large\", ax = ax)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "profiles[[\"n_posts\"]].plot.box(showfliers=False, fontsize = \"large\", ax = ax)\n",
    "plt.suptitle(\"Boxplots for numerical features (except for followers) in profiles dataset\", fontsize = 35)\n",
    "plt.tight_layout(pad = 1.5)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ADD HISTOGRAMS ON LOG SCALE\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "(profiles[\"following\"]+1).apply(np.log).hist(ax = ax, bins = 30)\n",
    "ax.set_xlabel(\"Number of users followed by users + 1 (log scale)\")\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "(profiles[\"n_posts\"]+1).apply(np.log).hist(ax = ax, bins = 30)\n",
    "ax.set_xlabel(\"Number of posts + 1 (log scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features of the location dataset [H2]\n",
    "# Description of the features\n",
    "# ...\n",
    "print(locations.columns.values)\n",
    "print(f\"Missing coordinates (percentage): {round(locations.lng.isna().sum()/len(locations), 5)}\")\n",
    "assert locations.lng.isna().sum() == locations.lat.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only meaningful thing that we can do with this dataset is plotting the locations on a world map\n",
    "\n",
    "# Swap latitude and longitude: in the dataset they are one in the place of the other\n",
    "locations[\"lat\"], locations[\"lng\"] = locations[\"lng\"], locations[\"lat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 8))\n",
    "ax = plt.gca()\n",
    "world = geopd.read_file(geopd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "world.plot(color=\"gray\", edgecolor = \"white\", ax = ax)\n",
    "plt.sca(ax)\n",
    "plt.scatter(locations[\"lng\"], locations[\"lat\"], color = \"red\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "# This is a clear case of overplotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we solve this? The simplest and probably the most effective way would be to cluster the data\n",
    "# K-means clustering with scikit\n",
    "kmeans_fit = clustering.KMeans(n_clusters=100, verbose=2, n_init=10, random_state=123, max_iter=50, tol = 0).fit(locations[[\"lng\", \"lat\"]].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centroids, get number of observations for each cluster (will be used as size of the point)\n",
    "centroids = kmeans_fit.cluster_centers_\n",
    "_ , weights = np.unique(kmeans_fit.labels_, return_counts=True)\n",
    "geocluster_df = pd.DataFrame(centroids, columns = [[\"lng\", \"lat\"]])\n",
    "standardization_factor_positions = np.std(weights)\n",
    "geocluster_df[\"weights\"] = weights/standardization_factor_positions*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 8))\n",
    "ax = plt.gca()\n",
    "world.plot(color=\"gray\", edgecolor = \"white\", ax = ax)\n",
    "plt.sca(ax)\n",
    "scatter_plot = plt.scatter(geocluster_df.iloc[:, 0], geocluster_df.iloc[:, 1], color = \"red\", alpha = 0.8, s = geocluster_df[\"weights\"])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "handles, labels = scatter_plot.legend_elements(prop=\"sizes\", alpha=0.8, color=\"red\", func = lambda x: x*standardization_factor_positions/50)\n",
    "\n",
    "plt.legend(handles, labels, title = \"Size | Number of loc.\")\n",
    "plt.suptitle(\"Centroids of the clusters emerging from the locations dataset\", fontsize = 20)\n",
    "plt.title(\"K-means used for the clustering, size of the centroids given by number of locations in each cluster\", fontsize = 13)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
