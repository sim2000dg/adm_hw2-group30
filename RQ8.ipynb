{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LinearRegression as regression_sk\n",
    "import os\n",
    "import math\n",
    "os.chdir(\"/Users/simonedigregorio/Downloads/archive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = pd.read_csv(\"instagram_posts.csv\", delimiter=\"\\t\", nrows = 1).columns.values.tolist()\n",
    "ignore_col = (\"description\", \"post_id\")\n",
    "posts = pd.read_csv(\"instagram_posts.csv\", delimiter=\"\\t\", parse_dates=[4],infer_datetime_format = True, usecols= [x for x in cols if x not in ignore_col])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a scatter plot of “likes” vs comments for posts\n",
    "# ylim and xlim are used in order to make the scatterplot a bit more readable\n",
    "posts_complete_lc = posts[[\"numbr_likes\", \"number_comments\"]].dropna()\n",
    "coeff, covariance_matrix = np.polyfit(posts_complete_lc[\"numbr_likes\"], posts_complete_lc[\"number_comments\"], deg = 1, cov = \"unscaled\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covariance_matrix[[0, 1], [0, 1]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.scatter(posts[\"numbr_likes\"], posts[\"number_comments\"])\n",
    "plt.ylabel(\"Number of comments\")\n",
    "plt.xlabel(\"Number of likes\")\n",
    "plt.xlim([0, 4*10**6])\n",
    "plt.ylim([0, 200000])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A way to check the linear (only the linear) relationship would be to use Pearson's correlation coefficient (i.e. standardized covariance). Intuitively, the amount of covariance between (the numbers of) likes and comments cannot be zero, so it makes sense that the correlation coefficient is significant (0.37), albeit not high.\n",
    "\n",
    "print(\"Pearson's Correlation coefficient for number of likes and the number of comments: {}\".format(round(posts[[\"numbr_likes\", \"number_comments\"]].corr().iloc[0, 1], 2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's check the relationship also with a linear regression\n",
    "posts_complete_lc = posts[[\"numbr_likes\", \"number_comments\"]].dropna()\n",
    "coeff, covariance_matrix = np.polyfit(posts_complete_lc[\"numbr_likes\"], posts_complete_lc[\"number_comments\"], deg = 1, cov = \"unscaled\")\n",
    "var1, var2 = covariance_matrix[[0, 1], [0, 1]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "ax = plt.gca()\n",
    "ax.axline((0, coeff[0]), slope = coeff[1], linewidth = 5, c = \"red\")\n",
    "ax.scatter(posts_complete_lc[\"numbr_likes\"], posts_complete_lc[\"number_comments\"])\n",
    "plt.xlim([0, 200])\n",
    "plt.xlabel(\"Number of likes\")\n",
    "plt.ylabel(\"Number of comments\")\n",
    "plt.ylim([0, 500])\n",
    "plt.show()\n",
    "\n",
    "# Print also p-value for the coefficients of the regression (simple t-test)\n",
    "# We assume homoscedasticity (since the covariance matrix of the coefficients uses the estimated variance of the residuals), which is a very unrealistic assumption in this case, but still. t-distribution at this point is basically the normal (due to the very high number of degrees of freedom), so we can use that and disregard the t.\n",
    "print(\"p-value for intercept: {}\".format(1-stats.norm.cdf((coeff[0]-0)/np.sqrt(var1))))\n",
    "print(\"p-value for slope: {}\".format(1-stats.norm.cdf((coeff[1]-0)/np.sqrt(var2))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the existence of a relationship between time and number of comments and likes\n",
    "# A good way to do that would be to discretize the time and then perform a chi-square test\n",
    "\n",
    "# Let's assume that if there is no relationship between likes/comments and time the distribution\n",
    "# of likes/comments is homogeneous across time, which makes sense. This is our null hypothesis. Under our null hypothesis, in more formal terms, the following behaviour emerges: the frequency (likes/comments) for the specific timeframe/hour is approximately distributed as a random variable centered on the expected frequency given by our null hypothesis, which is simply the total number of likes/comments. Since we have multiple timeframes/hours, we use the fact that the sum of n standardized squared normal r.v. is modelled by a chi-square with n degrees of freedom.\n",
    "# This is a one sided test because we are checking the right tail because this is a goodness of fit (it is not like a test of variance where we have to consider it two-sided)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\sum_{h=1}^{24}\\frac{\\left(f_h - \\frac{f_T}{24}\\right)^2}{\\frac{f_T}{24}} \\sim \\chi_{23}^2$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's do this in practice (likes)\n",
    "likes_per_hour = posts[\"numbr_likes\"].groupby(posts.cts.dt.hour).sum()\n",
    "test_statistic_chi = (((likes_per_hour-likes_per_hour.mean())**2)/likes_per_hour.mean()).sum()\n",
    "print(\"p-value for chi-squared test (likes): {}\".format(1-stats.chi2.cdf(test_statistic_chi, 23)))\n",
    "# It seems we can definitely reject our null hypothesis (the p-value is practically 0): the number of likes seems definitely not independent of the hour/timeframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's do this in practice (comments)\n",
    "comments_per_hour = posts[\"number_comments\"].groupby(posts.cts.dt.hour).sum()\n",
    "test_statistic_chi = (((comments_per_hour-comments_per_hour.mean())**2)/comments_per_hour.mean()).sum()\n",
    "print(\"p-value for chi-squared test (likes): {}\".format(1-stats.chi2.cdf(test_statistic_chi, 23)))\n",
    "# The same we said above applies for comments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To check which kind of relationship we are talking about, let's do some overkill:\n",
    "# Fourier Regression! Linear regression seems docile, but you can do literally whatever you want with the concept. One of the things is using a Fourier basis to transform the (single) predictor and fit a multivariate regression to estimate the Fourier coefficients. This is useful to estimate from real data the Fourier coefficients of an arbitrary periodic signal, such as the amount of likes during the day, something which we expect to be strongly periodic.\n",
    "\n",
    "hourly_mean_likes = posts[[\"cts\", \"numbr_likes\"]][posts[\"cts\"]>\"2013\"].groupby(pd.Grouper(key = \"cts\", freq=\"H\")).mean()\n",
    "hourly_mean_likes[\"time_int\"] = range(2, len(hourly_mean_likes)+2)\n",
    "hourly_mean_likes.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fit a linear regression to remove trend (if present)\n",
    "# The residuals are then used to fit the harmonic regression\n",
    "trend_regression = regression_sk(fit_intercept=True, copy_X=False)\n",
    "trend_regression = trend_regression.fit(np.expand_dims(hourly_mean_likes.time_int, axis = 1), hourly_mean_likes.numbr_likes)\n",
    "# Below, the residual vector aka fitted values vector\n",
    "residual_vector = trend_regression.predict(np.expand_dims(hourly_mean_likes.time_int, axis=1))-hourly_mean_likes.numbr_likes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, we need to transform the predictor in order to fit the harmonic regression. We compute cosine and sine terms from the univariate series. As can be seen from _armonic mult_, we only consider up to the second order of Fourier terms, which means 2 sine terms and two cosine terms\n",
    "\n",
    "armonic_mult = range(1, 11)\n",
    "\n",
    "fourier_design_matrix = [(np.cos((2*np.pi/24)* n * hourly_mean_likes.time_int.to_numpy()), np.sin((2*np.pi/24)* n * hourly_mean_likes.time_int.to_numpy())) for n in armonic_mult]\n",
    "\n",
    "fourier_design_matrix = [x for t in fourier_design_matrix for x in t]\n",
    "fourier_design_matrix = np.column_stack(fourier_design_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we fit the harmonic/fourier regression, since we already fitted a linear regression with intercept, and we are using the residuals we do not need to fit the intercept.\n",
    "fourier_regression = regression_sk(fit_intercept=False, copy_X=False)\n",
    "fourier_regression = fourier_regression.fit(fourier_design_matrix, residual_vector)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function is necessary for plotting the result\n",
    "def fourier_func(x:list[int], coeff:np.array, intercept:float, order:int, period:int) -> float:\n",
    "    output_list = list()\n",
    "    for time_index in x:\n",
    "        output = 0\n",
    "        for term_n in range(0, order):\n",
    "            a, b = coeff[0+2*term_n], coeff[1+2*term_n]\n",
    "            output += a*math.cos((2*math.pi/period)*(term_n+1)*time_index) + b*math.sin((2*math.pi/period)*(term_n+1)*time_index)\n",
    "        output_list.append(output + intercept)\n",
    "    return output_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fourier_func(list(range(1, 25)), fourier_regression.coef_, fourier_regression.intercept_, 10, 24)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(list(range(0, 25)), fourier_func(list(range(0, 25)), fourier_regression.coef_, fourier_regression.intercept_, 10, 24))\n",
    "plt.ylabel(\"Number of likes w.r.t. mean\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.xlim((0, 24))\n",
    "plt.grid(True)\n",
    "plt.title(\"Extraction of periodic signal with harmonic/Fourier regression\", fontdict={\"fontsize\": 25}, y = 1.05)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Distribution of followers\n",
    "\n",
    "# Let's analyze the distribution of followers.\n",
    "\n",
    "profiles = pd.read_csv(\"instagram_profiles.csv\", delimiter=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(profiles.followers.describe().iloc[1:].to_markdown())\n",
    "\n",
    "# kurtosis\n",
    "# extremely distorted distros\n",
    "\n",
    "# Again, as we have explained in RQ1, this is due to preferential attachment processes underlying social networks dynamics, which is even more intuitive, natural and clear when we think about the way profiles cluster in the directed social graph when looking at the edges represented by the follower connections. Intuitively, a new user is far more likely to follow a small set of users (VIPs) which have an already high number of followers, and this is basically the informal definition of a preferential attachment process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram showing highly probable log normality"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
